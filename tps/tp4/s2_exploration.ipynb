{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73f8bdf7",
   "metadata": {},
   "source": [
    "#  Exploration Sémantique du sous corpus  liés aux   articles de 1920 \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec9d6b2",
   "metadata": {},
   "source": [
    "## 1) Entités nommés\n",
    "je le fais pour  repérer les personnes, organisations et lieux les plus cités dans les articles 1920."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96f4b418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 PER :\n",
      "  Roi — 7\n",
      "  sach — 7\n",
      "  Blancs — 7\n",
      "  Rossel — 6\n",
      "  Etranger — 6\n",
      "  culs — 5\n",
      "  Dujardin — 5\n",
      "  Bru — 4\n",
      "  Louise — 4\n",
      "  qu’ — 4\n",
      "\n",
      "Top 10 ORG :\n",
      "  SOIR — 10\n",
      "  Chambre — 10\n",
      "  ANNONCES — 8\n",
      "  ABONNEMENTS — 8\n",
      "  PROVINCE — 7\n",
      "  Sénat — 6\n",
      "  Conseil — 6\n",
      "  DEM — 6\n",
      "  Eor — 5\n",
      "  Société des nations — 5\n",
      "\n",
      "Top 10 LOC :\n",
      "  Bruxelles — 101\n",
      "  qu’ — 39\n",
      "  Allemagne — 38\n",
      "  Belgique — 28\n",
      "  Anvers — 19\n",
      "  Paris — 17\n",
      "  Etat — 16\n",
      "  Berlin — 15\n",
      "  Angleterre — 14\n",
      "  France — 13\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os, re\n",
    "from collections import Counter, defaultdict\n",
    "import spacy\n",
    "\n",
    "DATA_DIR = Path(\"../data/txt\")\n",
    "DECADE = \"1920\"\n",
    "files_1920 = sorted([f for f in os.listdir(DATA_DIR) if f\"_{DECADE}-\" in f and f.endswith(\".txt\")])\n",
    "\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "def first_chunk(path, n_chars=4000):\n",
    "    return (path.read_text(encoding=\"utf-8\", errors=\"replace\"))[:n_chars]\n",
    "\n",
    "ent_counts = defaultdict(Counter)\n",
    "\n",
    "for f in files_1920:\n",
    "    doc = nlp(re.sub(r\"\\s+\", \" \", first_chunk(DATA_DIR / f)))\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in {\"PER\", \"ORG\", \"LOC\"} and len(ent.text) > 2:\n",
    "            ent_counts[ent.label_][ent.text.strip()] += 1\n",
    "\n",
    "for label in (\"PER\",\"ORG\",\"LOC\"):\n",
    "    print(f\"\\nTop 10 {label} :\")\n",
    "    for txt, c in ent_counts[label].most_common(10):\n",
    "        print(f\"  {txt} — {c}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89688ae",
   "metadata": {},
   "source": [
    "## 2)Sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b36a5308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Articles 1920 : 100\n",
      "Score moyen (≈ tonalité) : 0.29\n",
      "5 scores exemples : [-1.0, 0.6, -0.2, -0.2, 1.0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "positifs = {\n",
    "    \"accueil\",\"aide\",\"protection\",\"asile\",\"intégration\",\"travail\",\n",
    "    \"solidarité\",\"réfugiés\",\"droit\",\"citoyenneté\",\"naturalisation\" ,\"accueil\",\"protection\",\"asile\",\"intégration\",\"travail\",\"solidarité\",\n",
    "    \"droit\",\"citoyenneté\",\"naturalisation\",\"aide\",\"humanitaire\",\"secours\",\n",
    "    \"protéger\",\"emplois\",\"insertion\",\"étudier\",\"autoriser\",\"visa\"\n",
    "}\n",
    "negatifs = {\n",
    "    \"crainte\",\"crise\",\"illégal\",\"expulsion\",\"rejet\",\"xénophobie\",\n",
    "    \"trafic\",\"violence\",\"menace\",\"charge\",\"fraude\",\"encombrement\" ,\"peur\",\"tension\",\"fermer\",\"refuser\"\n",
    "}\n",
    "\n",
    "def tokenize_fr(text):\n",
    "    return re.findall(r\"[a-zàâçéèêëîïôûùüÿœ\\-]{3,}\", text.lower())\n",
    "\n",
    "def polarite(text):\n",
    "    toks = tokenize_fr(text)\n",
    "    p = sum(t in positifs for t in toks)\n",
    "    n = sum(t in negatifs for t in toks)\n",
    "    return (p - n) / max(1, p + n)\n",
    "\n",
    "texts_1920 = [(DATA_DIR / f).read_text(encoding=\"utf-8\", errors=\"replace\") for f in files_1920]\n",
    "scores = [polarite(t) for t in texts_1920]\n",
    "\n",
    "print(\"Articles 1920 :\", len(scores))\n",
    "print(\"Score moyen (≈ tonalité) :\", round(float(np.mean(scores)), 3))\n",
    "print(\"5 scores exemples :\", [round(float(s),3) for s in scores[:5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8fb03c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1006)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pip install -q scikit-learn nltk\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac747956",
   "metadata": {},
   "source": [
    "## 3) bigrams \n",
    "l'objectif c'st de voir les paires de mots les plus fréquentes, après stopwords. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ff95d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichiers 1920 : 998\n",
      "Documents tokenisés : 998\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, re\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "DATA_DIR = Path(\"./data/txt\")\n",
    "\n",
    "files_1920 = [f for f in sorted(os.listdir(DATA_DIR)) if \"_1920\" in f and f.endswith(\".txt\")]\n",
    "print(\"Fichiers 1920 :\", len(files_1920))\n",
    "\n",
    "def tokenize(txt):\n",
    "    return re.findall(r\"[a-zàâçéèêëîïôûùüÿœ]{3,}\", txt.lower())\n",
    "\n",
    "STOP = {\n",
    "    \"a\",\"à\",\"au\",\"aux\",\"avec\",\"car\",\"ce\",\"cela\",\"ces\",\"cette\",\"chaque\",\"ci\",\"comme\",\"d\",\"dans\",\"de\",\"des\",\"du\",\n",
    "    \"elle\",\"en\",\"et\",\"être\",\"eux\",\"il\",\"ils\",\"je\",\"la\",\"le\",\"les\",\"leur\",\"lui\",\"ma\",\"mais\",\"mes\",\"moi\",\"mon\",\n",
    "    \"ne\",\"nos\",\"notre\",\"nous\",\"on\",\"ou\",\"où\",\"par\",\"pas\",\"pour\",\"qu\",\"que\",\"qui\",\"sa\",\"se\",\"ses\",\"sans\",\"son\",\n",
    "    \"sur\",\"ta\",\"te\",\"tes\",\"toi\",\"ton\",\"tu\",\"un\",\"une\",\"vos\",\"votre\",\"vous\",\"y\",\"ça\",\"cest\",\"cet\",\"dont\"\n",
    "}\n",
    "\n",
    "docs = []\n",
    "for fn in files_1920:\n",
    "    with open(DATA_DIR / fn, encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "        docs.append(tokenize(f.read()))\n",
    "\n",
    "print(f\"Documents tokenisés : {len(docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9cf54728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ont été  -> 2258\n",
      "aujourd hui  -> 1875\n",
      "après midi  -> 940\n",
      "ordre jour  -> 814\n",
      "avait été  -> 778\n",
      "chemins fer  -> 720\n",
      "aura lieu  -> 702\n",
      "après avoir  -> 688\n",
      "etats unis  -> 664\n",
      "point vue  -> 623\n",
      "autre part  -> 617\n",
      "lloyd george  -> 526\n",
      "est plus  -> 514\n",
      "agence rossel  -> 490\n",
      "tout faire  -> 477\n",
      "maison commerce  -> 462\n",
      "jeune fille  -> 457\n",
      "avant guerre  -> 454\n",
      "pendant guerre  -> 449\n",
      "février heures  -> 448\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# docs = liste de documents tokenisés (tu l'as déjà)\n",
    "bigrams = Counter((d[i], d[i+1]) for d in docs for i in range(len(d)-1))\n",
    "\n",
    "# filtre : on enlève les bigrams qui contiennent un stopword\n",
    "def is_clean(bg): \n",
    "    return bg[0] not in STOP and bg[1] not in STOP\n",
    "\n",
    "top20 = [(a,b,c) for (a,b),c in bigrams.most_common(200) if is_clean((a,b))][:20]\n",
    "for a,b,c in top20:\n",
    "    print(f\"{a} {b}  -> {c}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1534d5c",
   "metadata": {},
   "source": [
    "## 4) Collocations fortes (PMI)\n",
    "je fais ca pour repérer les paires “surreprésentées” au-delà de la simple fréquence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "febaa930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Collocations PMI autour de « immigration » ===\n",
      "Gauche : []\n",
      "Droite : []\n",
      "\n",
      "=== Collocations PMI autour de « étranger » ===\n",
      "Gauche : ['vinco', 'vlnce', 'tributaire', 'vince', 'achetées', 'tributaires', 'joug', 'disponibilités', 'réfugié', 'envoyée']\n",
      "Droite : ['dangereux', 'exposer', 'tarif', 'dublin', 'ouvre', 'revenu', 'centimes', 'soient', 'nombreux', 'aide']\n",
      "\n",
      "=== Collocations PMI autour de « asile » ===\n",
      "Gauche : ['interné', 'droit', 'donner', 'œuvre']\n",
      "Droite : ['indignement', 'aliénés', 'accordé', 'ces', 'sur', 'aux', 'des']\n",
      "\n",
      "=== Collocations PMI autour de « réfugié » ===\n",
      "Gauche : ['était']\n",
      "Droite : ['hollande', 'étranger', 'déclaré', 'dans']\n",
      "\n",
      "=== Collocations PMI autour de « séjour » ===\n",
      "Gauche : ['assigne', 'court', 'beau', 'permis', 'lille', 'années', 'long', 'mon', 'son', 'leur']\n",
      "Droite : ['lucerne', 'esneux', 'auprès', 'bas', 'paris', 'avenue', 'trois', 'dans', 'bruxelles', 'aux']\n",
      "\n",
      "=== Collocations PMI autour de « permis de séjour » ===\n",
      "Gauche : []\n",
      "Droite : []\n",
      "\n",
      "=== Collocations PMI autour de « titre de séjour » ===\n",
      "Gauche : []\n",
      "Droite : []\n",
      "\n",
      "=== Collocations PMI autour de « carte d’identité d’étranger » ===\n",
      "Gauche : []\n",
      "Droite : []\n",
      "\n",
      "=== Collocations PMI autour de « visa » ===\n",
      "Gauche : ['formalité', 'par']\n",
      "Droite : ['unique', 'pour']\n",
      "\n",
      "=== Collocations PMI autour de « passeport » ===\n",
      "Gauche : []\n",
      "Droite : []\n",
      "\n",
      "=== Collocations PMI autour de « laissez-passer » ===\n",
      "Gauche : []\n",
      "Droite : []\n",
      "\n",
      "=== Collocations PMI autour de « frontière » ===\n",
      "Gauche : ['contrôlé', 'franchir', 'prolongée', 'neutralisation', 'franchi', 'traversé', 'journellement', 'prussienne', 'annoncent', 'fraude']\n",
      "Droite : ['herbestal', 'arménienne', 'lithuanienne', 'hollandaise', 'attaquant', 'roumaine', 'bessarabie', 'hollando', 'prussienne', 'lithuanie']\n",
      "\n",
      "=== Collocations PMI autour de « douanes » ===\n",
      "Gauche : ['directes', 'des', 'les']\n",
      "Droite : ['accises', 'fonctionnent', 'classe', 'sont', 'des', 'les']\n",
      "\n",
      "=== Collocations PMI autour de « contrôle » ===\n",
      "Gauche : ['affichés', 'établissant', 'partager', 'associés', 'postale', 'contenu', 'interalliée', 'faciliter', 'supérieur', 'soumettre']\n",
      "Droite : ['formées', 'sévère', 'médical', 'volant', 'cerveau', 'médecin', 'exploitation', 'ravitaillement', 'établi', 'nécessaire']\n",
      "\n",
      "=== Collocations PMI autour de « expulsion » ===\n",
      "Gauche : ['ordonnant']\n",
      "Droite : ['tous', 'des']\n",
      "\n",
      "=== Collocations PMI autour de « refoulement » ===\n",
      "Gauche : []\n",
      "Droite : []\n",
      "\n",
      "=== Collocations PMI autour de « internement » ===\n",
      "Gauche : ['examiner', 'kaiser', 'leur']\n",
      "Droite : ['réfugiés', 'kaiser']\n",
      "\n",
      "=== Collocations PMI autour de « administration des étrangers » ===\n",
      "Gauche : []\n",
      "Droite : []\n",
      "\n",
      "=== Collocations PMI autour de « police des étrangers » ===\n",
      "Gauche : []\n",
      "Droite : []\n",
      "\n",
      "=== Collocations PMI autour de « office des réfugiés » ===\n",
      "Gauche : []\n",
      "Droite : []\n",
      "\n",
      "=== Collocations PMI autour de « consulat » ===\n",
      "Gauche : ['solennelle', 'soigné', 'réouverture', 'septembre', 'agents', 'adresser']\n",
      "Droite : ['breslau', 'punition', 'polonais', 'français', 'général', 'france']\n",
      "\n",
      "=== Collocations PMI autour de « ministère de l’intérieur » ===\n",
      "Gauche : []\n",
      "Droite : []\n",
      "\n",
      "=== Collocations PMI autour de « apatride » ===\n",
      "Gauche : []\n",
      "Droite : []\n",
      "\n",
      "=== Collocations PMI autour de « demandeur d’asile » ===\n",
      "Gauche : []\n",
      "Droite : []\n",
      "\n",
      "=== Collocations PMI autour de « rapatriement » ===\n",
      "Gauche : ['suppression']\n",
      "Droite : ['des']\n",
      "\n",
      "=== Collocations PMI autour de « déplacés » ===\n",
      "Gauche : []\n",
      "Droite : []\n",
      "\n",
      "=== Collocations PMI autour de « secours » ===\n",
      "Gauche : ['appela', 'précipitèrent', 'précipita', 'appelait', 'rétablir', 'porter', 'pacte', 'chercher', 'porta', 'caisses']\n",
      "Droite : ['mutuels', 'immédiats', 'religion', 'chômage', 'vivres', 'détachement', 'suppose', 'alimentation', 'navire', 'venus']\n",
      "\n",
      "=== Collocations PMI autour de « réinstallation » ===\n",
      "Gauche : []\n",
      "Droite : []\n",
      "\n",
      "=== Collocations PMI autour de « main-d’œuvre étrangère » ===\n",
      "Gauche : []\n",
      "Droite : []\n",
      "\n",
      "=== Collocations PMI autour de « recrutement » ===\n",
      "Gauche : ['concerne', 'commission', 'bureau', 'loi', 'pour', 'que']\n",
      "Droite : ['dessinateurs', 'techniciens', 'personnel', 'armée', 'votre', 'pour', 'une', 'est']\n",
      "\n",
      "=== Collocations PMI autour de « logement » ===\n",
      "Gauche : ['nourriture', 'crise', 'locataire', 'coopérative', 'problème', 'salaires', 'maisons', 'question', 'maison', 'pour']\n",
      "Droite : ['équivalent', 'gratuit', 'chauffage', 'jardin', 'ecrire', 'rue', 'des', 'dans', 'est', 'les']\n",
      "\n",
      "=== Collocations PMI autour de « ouvriers étrangers » ===\n",
      "Gauche : []\n",
      "Droite : []\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "uni = Counter(t for d in docs for t in d)\n",
    "N_uni = sum(uni.values())\n",
    "N_bi  = sum(bigrams.values())\n",
    "\n",
    "def pmi(w1, w2, min_count=3):\n",
    "    c12 = bigrams[(w1,w2)]\n",
    "    if c12 < min_count: \n",
    "        return None\n",
    "    p12 = c12 / N_bi\n",
    "    p1  = uni[w1] / N_uni\n",
    "    p2  = uni[w2] / N_uni\n",
    "    return math.log2(p12 / (p1*p2))\n",
    "\n",
    "targets = [\"immigration\",\"étranger\",\"asile\",\"réfugié\", \"séjour\", \"permis de séjour\", \"titre de séjour\", \"carte d’identité d’étranger\",\n",
    "    \"visa\", \"passeport\", \"laissez-passer\", \"frontière\", \"douanes\", \"contrôle\",\n",
    "    \"expulsion\", \"refoulement\", \"internement\",\n",
    "    \"administration des étrangers\", \"police des étrangers\", \"office des réfugiés\",\n",
    "    \"consulat\", \"ministère de l’intérieur\",\n",
    "    \"apatride\", \"demandeur d’asile\",\n",
    "    \"rapatriement\", \"déplacés\", \"secours\", \"réinstallation\",\n",
    "    \"main-d’œuvre étrangère\", \"recrutement\", \"logement\", \"ouvriers étrangers\"\n",
    "]\n",
    "\n",
    "for t in targets:\n",
    "    R = sorted([(w2, pmi(t,w2)) for (w1,w2) in bigrams if w1==t and pmi(t,w2) is not None],\n",
    "               key=lambda x: -x[1])[:10]\n",
    "    L = sorted([(w1, pmi(w1,t)) for (w1,w2) in bigrams if w2==t and pmi(w1,t) is not None],\n",
    "               key=lambda x: -x[1])[:10]\n",
    "    print(f\"\\n=== Collocations PMI autour de « {t} » ===\")\n",
    "    print(\"Gauche :\", [w for w,_ in L])\n",
    "    print(\"Droite :\", [w for w,_ in R])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
