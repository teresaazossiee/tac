{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings : le modèle Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement et traitement des phrases du corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création d'un objet qui *streame* les lignes d'un fichier pour économiser de la RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySentences(object):\n",
    "    \"\"\"Tokenize and Lemmatize sentences\"\"\"\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "\n",
    "    def __iter__(self):\n",
    "        for line in open(self.filename, encoding='utf-8', errors=\"backslashreplace\"):\n",
    "            yield [unidecode(w.lower()) for w in wordpunct_tokenize(line)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.MySentences'>\n",
      "/System/Volumes/Data/Users/azossieteresafabiola/Tp3/tac/tps/data/txt/sents.txt\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "infile = \"/System/Volumes/Data/Users/azossieteresafabiola/Tp3/tac/tps/data/txt/sents.txt\"\n",
    "sentences = MySentences(infile)\n",
    "print(type(sentences))\n",
    "\n",
    "import os\n",
    "print(infile)\n",
    "print(os.path.exists(infile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = \"/System/Volumes/Data/Users/azossieteresafabiola/Tp3/tac/tps/data/txt/sents.txt\"\n",
    "sentences = MySentences(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/azossieteresafabiola/Tp3/tac/tps/tp3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for root, dirs, files in os.walk(os.getcwd(), topdown=True):\n",
    "    if \"sents.txt\" in files:\n",
    "        print(\"FOUND:\", os.path.join(root, \"sents.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOUND: /System/Volumes/Data/Users/azossieteresafabiola/Tp3/tac/tps/data/txt/sents.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "for root, dirs, files in os.walk(\"/\", topdown=True):\n",
    "    if \"sents.txt\" in files:\n",
    "        print(\"FOUND:\", os.path.join(root, \"sents.txt\"))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/System/Volumes/Data/Users/azossieteresafabiola/Tp3/tac/tps/data/txt/sents.txt\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "infile = \"/System/Volumes/Data/Users/azossieteresafabiola/Tp3/tac/tps/data/txt/sents.txt\"\n",
    "sentences = MySentences(infile)\n",
    "\n",
    "import os\n",
    "print(infile)\n",
    "print(os.path.exists(infile))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Détection des bigrams\n",
    "\n",
    "Article intéressant sur le sujet : https://towardsdatascience.com/word2vec-for-phrases-learning-embeddings-for-more-than-one-word-727b6cf723cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_phrases = Phrases(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'object `phrases` peut être vu comme un large dictionnaire d'expressions multi-mots associées à un score, le *PMI-like scoring*. Ce dictionnaire est construit par un apprentissage sur base d'exemples.\n",
    "Voir les références ci-dessous :\n",
    "- https://arxiv.org/abs/1310.4546\n",
    "- https://en.wikipedia.org/wiki/Pointwise_mutual_information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(bigram_phrases.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il contient de nombreuses clés qui sont autant de termes observés dans le corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15850647"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bigram_phrases.vocab.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prenons une clé au hasard :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1q\n"
     ]
    }
   ],
   "source": [
    "key_ = list(bigram_phrases.vocab.keys())[144]\n",
    "print(key_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le dictionnaire indique le score de cette coocurrence :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "488"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_phrases.vocab[key_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lorsque l'instance de `Phrases` a été entraînée, elle peut concaténer les bigrams dans les phrases lorsque c'est pertinent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion des `Phrases` en objet `Phraser`\n",
    "\n",
    "`Phraser` est un alias pour `gensim.models.phrases.FrozenPhrases`, voir ici https://radimrehurek.com/gensim/models/phrases.html.\n",
    "\n",
    "Le `Phraser` est une version *light* du `Phrases`, plus optimale pour transformer les phrases en concaténant les bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_phraser = Phraser(phrases_model=bigram_phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le `Phraser` est un objet qui convertit certains unigrams d'une liste en bigrams lorsqu'ils ont été identifiés comme pertinents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction des trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous répétons l'opération en envoyant cette fois la liste de bigrams afin d'extraire les trigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_phrases = Phrases(bigram_phraser[sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trigram_phraser = Phraser(phrases_model=trigram_phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création d'un corpus d'unigrams, bigrams, trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = list(trigram_phraser[bigram_phraser[sentences]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['mi', 'imnri', 'r', 'i', '<<', 'i', 'i', 'hmu', \"'\", 'i', '/', 'tx', \"-'\", 'l', ':', 'marche', 'tenu', 'hors', 'villa', ',', 'la', '9', '.'], ['--', 'u', 'a', 'ete', 'vaain', 'si', 'teicj', '>>', 'm', 'races_indigenes', 'de', 'fr', '.'], ['31', '<)', 'a', '5s', \"'\", 'k', '131', 'de', '.'], ['rasa', 'iichakdui', \"'\", 'te', ',', 'do', '(', 'r', '.', '3s0', 'h', '710', '.', 'taureaux', 'iallsenas', ',>', 'ia', 'u', '\\\\', '--', 'a', '--', ';', '0ii', '.'], ['hollandais', ',', 'dufr', '.'], ['0', '.'], ['--', 'a', '9', '.--', 'la', 'idto', '-', 'vachei', 'laitieres', ':', 'bn', 'vante', '1q', '.'], ['vendues', '3', '\\\\', 'au', 'prix', 'la', '410', 'a', '*', '<<', 'i', 'h', '\\\\;', 'genisses', ',', 'kl', '.'], [\"'.\", '9', '.'], ['i', 'l', '.', '2', 'i', '.', 'id', '.'], ['da', '370', 'i', '6lutr', '.'], ['marche', 'a', '<', 'u', 'porcs', '.'], ['--', 'categorie', 'de', 'lt', 'ilashtya', ':', \"'\", '237', 'on', 'vente', ';', 'vendus', '1', 'm', '.', 'do', \"'\", '2', 'i', '.--', 'a', ';:,', 'l', '--;', 'i', '.', 'l', '.'], ['des', 't', \"'\", 'innlrov', '-', 'i3ie', ';>>', 'vente', ',', 'vendus', '93', '.', 'de', '32', '.--', 'a', '52', '.--.'], ['xumiir', '.'], ['10', 'a', '*', 'v', '.--_froment', '>>', 'las', '190', 'kit', '..', 'fr', '.'], ['15', '.', '23', ';', 'mi', \"'\", 'te', '>>,', '--;', 'epautre', '.', 'l3', '.--', ';', 'seigle', '.'], ['14', '.', '5', '>;', 'avoine', '.'], ['10', '.', '30', ';', 'orge', '.'], ['--;_feveroles', ',', '22', '.--;_pommes', 'do', 'terre', '.'], ['g', '.--;_paille', ',', 'v', '.', '--;', 'fcin', '.', '6', '.', '50', ':', 'lioumon', ',', '--.'], ['2', \"%'\", 'ivellom', '.'], ['10', 'mv', '.'], ['--', 'froment', ',', 'las', '109_kll', '.,', '(', 'r', '.', '13', '.'], ['--', '\\\\', '--;_selgle', '.', 'ii', '.-;\\\\', '--;_avoine', ',', '16', '.', '509', '-;', 'orge', ',', '17', '30', 'j', '--,--;_escourgeon', '.'], ['#', 'inint', '-', 't', '>>*', 'o', '<<', 'd', ',', '10', 'nov', '.', '--', 'froment', 'je', 'culture', ',', 'par', '400kil', '.,', 'fr', '.'], ['10', '.--', '1', '--.--;', 'm', '.', 'commercial', ',', '16', '.'], ['--', 'a', '--.--,_seigle', '.'], ['13', '.', '50', 'a', '--;_avoine', '.'], ['10', '.', '50', 'a', '--.--;_orge', '.'], ['17', '.--', 'a', '--;_farine', 'ira', 'de', 'froment', ',', '0', '.--', 'a', '0', '.'], ['--;', 'son', 'gros', 'de', 'tco', '*', 'ment', ',', '15', '.--', 'a', '--.--.'], ['huiie', 'de', 'colza', ',', 'par', 'hcctoi', '.,', '--', 'a', '--;', 'u', '.', 'epures', ',', '--~', 'a', ';', 'huile', 'de', 'lin', '.'], ['--,--', 'a', '--;_tourteaux', 'de', 'colza', 'par', '100_kll', '.,', '--', 'a', '--;_id', '.'], ['de', 'lin', ',', '--', 'a', 'i', 'pommes', 'de', 'terre_blanches', ',', '6', '.', '3', '>>', 'a', '0', '.--.'], ['...', ',,', ',,,,', 'paille', ',', 'fi', '.', '50', 'a', '0', '.--;', 'loln', \",'\", '5', '.', '5i', ')', 'a', '0', '.--;_beurre', ',', 'le', 'kilo', '.'], ['2', '.', 'o', 'a', '0', '.--;_oeufs', ',', 'les', '26', ',', '3', '.', '30', 'a', '0', '.--;_genievre', ',', 'l', \"'\", 'uect', '.,', '--', 'a', 'esprit', ',', '0', '.--', 'a', '0', '.--.'], ['tournai', ',', '10', 'nov', '.', '--', 'froment_blaze', ',', 'l', \"'\", 'iiaetoluri', ',', 'fr', '.'], ['--.--', 'a', '--;_froment', ',--', 'u', '--;_metell', ',', '--', 'a', '--;_seigle', ',--.--', 'a', '--;_avoine', ',', '--', 'a', '--;', 'feve', '-', 'roles', ',', '--.--', 'a', '--;_beurre', ',', 'le', 'kilo', ',', '2', '.', '00', 'a', '3', ',', '10', ';', 'oeufs', ',', 'les', '26', ',', '3', '.', '70', 'u3', '.', 'c0', '.'], ['wacrofiliom', ',', '10', 'nov', '.--', 'beurre', '.', 'le', '1', '/', '3', 'il', '-,', '1', '.', '5qa', '1', '.', '60', ',', 'oeufs', ',', 'les', '26', ',', '3', '.', '99', 'a', '1', '.', '16', ';', 'lin', ',', 'les', '100_kilos', ',', '93', '.--', 'a', '145', '.--;_etoupes', ',', 'lee', '1u0', 'kii', '.,', '30', '.--', 'a', '35', '.--;_pommes', 'de', 'terre', ',', 'les', '100', '>', 'il', '.,', '3', '.--', 'a', '3', '.', '50', ';', 'jeune', 'porc', ',', 'la', 'piece', ',', '33', '.--', 'a', '50', '.--;_lapin', '.'], ['2', '.--', 'a', '3', '.', '95', ';', 'poulet', ',', '2', '.', '25', 'a', '3', '.'], ['--.'], ['vin', '-', 'e', '<<,', '10', 'nov', '.', '--', 'froment', ',', 'les', '10j', 'kll', '.,', 'fr', '.'], ['18', '.--', 'a', '19', '.', '59', ';', 'seigle', ',', '16', '.-', 'a', '16', '.', '75', ';', 'avoine', ',', '16', '.--', 'a', '16', '.', '75', ':', 'pois', '--', 'a', '--;_feveroles', ',', '--.--', 'a', '--.--;_pommes', 'de', 'terre', ',', '8', '80', 'a', 'g', '.--*,', 'beurre', ',', 'le', 'kilo', ',', '2', '.', '90', 'a', '3', '.', '20', ';', 'oeufs', ',', 'le', '(', 'iuart3', '-', 'ron', '.'], ['i', '.', '16', 'a', '4', '.', '63', '.'], ['charmanne', '.'], ['consul_general', 'de', 'belgique', 'a', 'ottawa', '(', 'canada', ').', 'a', 'ele', 'nomme_consul', 'general', 'a', 'bang_kok', '(', 'siam', '),', 'avec', 'juridiction', 'sur', 'le', 'siam', 'et', 'les', 'slrails', 'seluements', '.'], ['--', 'm', '.', 'kelels', ',', 'consul', 'de', 'belgique', 'a', 'tien', '-', 'tsin', '(', 'chine', '),', 'a', 'ele', 'nomme_consul', 'a', 'ottawa', '(', 'canada', '),', 'avec', 'juridiction', 'sur', 'la', 'federation', 'canadienne', 'et', 'ja', 'colonie', 'de', 'terre', '-', 'neuve', '.'], ['decoration', 'c', \".'\", 'v', \"'\", 'gae', '.'], ['--', 'la', '.', 'decoration_civique', 'a', 'ele', 'decernee', 'aux', 'agents', 'de', 'l', \"'\", 'administration', 'des', 'postes', ';', 'designes_ci', '-', 'apres', ',', 'savoir', ':', 'la', 'medaille', 'de', 'ire_classe', ':', 'a', 'm', '.', 'billy', '.'], ['facteur_rural', '.'], ['la', 'medaille', 'de', '2e_classe', ':', 'a', 'm', '.', 'goringy', 'facteur', 'local', '.', '.', 'armee', '.'], ['--', 'le', 'sergent', ',', 'en', 'conge_illimite', ',', 'baudouin', '\"', 'est', 'nomme', 'sous', '-', 'lieutenanl', 'payeur', 'de', 'reserve', '.'], ['notariat', '.'], ['--', 'sont_acceptees', 'les', 'demissions', 'de', 'm', '.', 'myin', '.'], ['de', 'ses_fonctions', 'de', 'notaire', 'a', 'la', 'residence', 'd', \"'\", 'anvers', ';', 'do', 'm', '.', 'jadol', ',', 'id', '.'], ['de', 'marche', '.'], ['ecole_militaire', '.'], ['--', 'sont', 'admis', 'a', 'l', \"'\", 'ecole_militaire', ',', 'en', 'qualite', 'd', \"'\", 'eleves', 'de', 'la', '57e', 'promotion', 'de', 'l', \"'\", 'infanterie', 'et', 'ao', 'la', '<', \"'\", '*', \"'\", '*', '-', '\"', 'i', 'cavalerie', ',', 'les', 'jeunes_gens', 'dont', 'les', 'noms_suivent', ':', 'hanon', 'de', 'louvet', ',', 'van_sprang', ',', 'deyloo', ',', 'petit', ',', 'champagne', '.'], ['masui', ',', 'gerard', ',', 'vanneste', ',', 'lallemand', ',', 'van', 'iioeeke', \".'\", 'rigano', '.'], ['ilendrickx', ',', 'von', 'glaboke', ',', 'sou', '-', \"'\", 'moy', '.', 'couturieaux', ',', 'lucion', ',', 'mersch', ',', 'iledo', ',', 'iloudmont', ',', 'de', 'heusch', '.'], ['terfve', '.'], ['labio', ',', 'brabant', ',', 'franckx', ',', 'foulon', ',', 'fiahiiifirh', '.'], ['simmi', '\"', 'tfannatvnin', 'fivnv', '^', 'f', 'h', '-.', 'ilu', '/', 'phnin', '.'], ['u', 'gernaert', ',', 'simon', \",'\", 'henncquin', ',', 'fcro', \"'\", 't', ',', 'g', '->', 'ile', ',', 'i', \"'\", 'orjo', 'e', \"'\", 'y', 'hannus', '.', 'noel', ',', 'flanieng', ',', 'bri', '-', 'matchovelette', ',', 'gondry', ',', 'doux', ',', 'vermeuleu', ',', 'giilo', 'gillot', ',', 'boufvin', '.'], ['borremans', '.'], ['academie_royale_flamande', 'de', 'langue', 'et', 'de', 'litterature', '.'], ['--', 'l', \"'\", 'election', 'faite', 'par', 'l', \"'\", 'academie_flamande', ',', 'dans', 'sa', 'seance', 'du', '17', 'octobre_1906', ',', 'de', 'm', '.', 'le', 'docteur', 'hugo_verriest', ',', 'a', 'ingoyghem', ',', 'en', 'qualite', 'de', 'membre_effectif', ',', 'en', '-', 'remplacement', 'de', 'feu', 'm', '.', 'janssens', ',', 'est_approuvee', '.'], ['sapeurs', '-', 'pompiers_communaux', 'armes', '.'], ['--', 'm', '.', 'pa', '*', 'njels', ',', 'sous', '-', 'fieu', 'tenant', 'au', 'corps', 'arme', 'de', 'sapeurs', '-', 'pom', '-', 'iers', 'communaux', 'de', 'schaerheek', ',', 'est', 'nomme', 'lieule', '-', 'p', 'u', 'teuant', ',-', 'en_remplacement', 'de', 'm', '.', 'verteneuil', ',', 'decede', '.'], ['enseignement_moyen', '.'], ['--', 'mme', 'schaefer', '-', 'misonne', ',.'], ['.', 'directrice', 'a', 'titre_provisoire', 'de', 'l', \"'\", 'ecole_moyenne', 'de', 'l', \"'\", 'elat', 'pour', 'alles', ',', 'a', 'jumet', ',', 'est', 'dechargee', 'des', 'fonciions', 'de', 'regente', 'd', \"'\", 'economie', 'domes', '!'], ['ique', 'a', 'l', \"'\", 'ecole', '\"', 'moyen', 'ne', 'del', \"'\", 'ktai', 'pour', 'ailes', ',', 'a', 'la', 'louviere', '.'], ['--', 'm', '.', 'barzin', 'est', 'decharge', ',', 'sur', 'sa', 'demande', ',', 'des', 'fonctions', 'de', 'regent', 'a', 'l', \"'\", 'ecole_moyenne', 'de', 'l', \"'\", 'etal', 'pour', \"'\", 'garcons', ',', 'a', 'spa', ',', 'avec', 'autorisation', 'd', \"'\", 'en', 'conserver', 'le', 'r', 'titre_honorifique', ':', 'il', 'est', 'admis', 'a', 'faire_valoir', 'ses_droits', ';', 'a', 'ja', 'pension', '.'], [';', \"'\", '-', 'r', '-', 'm', '.', 'drainer', ',', 'directeur', 'a', 'titre_provisoire', 'de', 'l', \"'\", 'ecole', 'moyenue', 'de', 'l', \"'\", 'etat', 'pour', '.', 'garcons', '^', 'a', 'wavre', ',', 'est', 'decharge', ',', 'sur', 'sa', 'demande', ',', 'des', 'fonctions', 'de', 'professeur', 'de', 'gymnastique', ',', 'en', ',', 'partage', ',', 'a', 'l', \"'\", 'ecole_moyenne', 'do', 'l', \"'\", 'etat', 'pour', 'garcons', ',', 'a', 'roeulx', '.'], ['*', 'contributions_directes', ',', 'douanes', 'et', 'accises', '.'], ['--', 'm', '.'], ['du', '-', '#', 'val', ',', 'receveur', 'des', 'contributions_directes', 'et', 'des', 'accises', 'a', 'deynze', '.', 'est', 'admis', ',', 'sur', 'sa', 'demande', ',', 'a', 'faire_valoir', 'ses_droits', 'a', 'la', 'pension', 'de', 'retraite', '.'], ['coinmission', 'tnedicale', 'provinciale', '.'], ['--', 'sont_nommes', 'membres_correspondants', 'de', 'la', 'commission_medicale', 'piminciale', 'de', 'namur', ';', 'mm', '.'], ['les', 'doctonrs', 'rolin', ',', 'u', \"'\", 'e', 'fosses', ';', 'lebrun', ',', 'de', 'ligny', ';', 'giliard', ',', 'do', 'ctiampion', ',', 'et', '.'], ['defasse', ',', 'de', 'spj', '\\\\', 'en_remplacement', 'de', 'mm', ',', 'les', 'docteurs', '.'], ['wery', ',', 'de', 'fosses', ',', 'et', 'fermine', ',', 'do', 'lignv', ',', 'demis', '-', ';', 'sionnairos', ',', 'et', 'de', 'mm', '.'], ['jes', 'docteurs', 'renard', ',', \"'\", 'de', 'champion', 'et', 'dehaybe', ',', 'de', 'spy', ',', 'decedes', '.'], ['arts', ',', 'sciences', 'et', 'lettres', 'declamation', '.', 'et', 'diction', '.'], ['*', '--', 'on', 'nous', 'demande', 'de', 'divers_cotes', 'des', 'renseignements', 'sur', 'le', 'cours', 'que', 'va', 'donner', 'm', '.', 'hittemans', '.'], ['nous', 'no', 'croyons_pouvoir', 'mieux', 'faire', 'que', 'd', \"'\", 'engager', 'les', 'interesses', 'a', 's', \"'\", 'adresser', 'a', 'l', \"'\", 'artiste', ',', 'rue', 'verhulst', ',', '6', ',', 'a', 'uccle', '.'], ['universite_libre', ',', 'rue', 'des', 'sols', '.'], ['--', 'demain_lundi', ',', 'a', 'r', 'meures', 'du', 'soir', ',', 'conference', 'par', 'm', '.', 'lameere', ':', '<<', 'la', 'fondation', '>>.'], ['universite_populaire', 'd', \"'\", 'euerbeek', '.'], ['--', 'lundi', '12', ',', 'a', '3', 'h', '.', '1', '/', '2', ',', 'au', 'local', ',', '4', ',', 'rue', 'do', 'l', \"'\", 'etang', ',', 'inauguration_solennelle', '.'], ['discours', 'de', 'm', '.', 'e', '.', 'richard', '.'], ['conference', 'de', 'xi', '.', 'ch', '.', 'buls', '.', 'sujet', ':', '<<-', 'la', 'corse', '>>.'], ['projections_lumineuses', '*', 'universite_populaire', 'de', 'saint', '-', 'josse', ',', '67', ',', 'rue', 'de', 'la', 'limite', '.'], ['demain_lundi', ',', 'a', '8', 'h', '.', '1', '/', '4', ',', 'eauserio', 'litteraire', ':', '<<', 'maxim', 'gorki', '<<.', 'lectures', '.'], ['vamicale', 'de', 'vecole', 'n_deg', '7', '.'], ['--', 'demain_lundi', ',', 'a', '8', 'h', '.', 'du', 'soir', ',', 'dans', 'le', 'preau', 'de', 'l', \"'\", 'ecole', 'n_deg', '7', ',', 'rue_haute', '225', \"'\", 'conference', 'par', 'madame', 'journaux', ':', '<<', 'de', 'la', 'grande', '-', 'chartreuse', 'a', 'ja', 'cote', 'd', \"'\", 'azur', '>>.', 'projections_lumineuses', '.'], ['foyer_intellectuel', ',', 's0', ',', 'rue', 'du', 'fort', '.'], ['--', 'demain_lundi', ',', 'a', '8b', ',,', 'm', '.', 'j', '.', 'vincent', ':', '<<', 'la', 'meteorologie', ':', 'le', 'barometre', '>>.']]\n"
     ]
    }
   ],
   "source": [
    "print(corpus[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrainement d'un modèle Word2Vec sur ce corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 37min 16s, sys: 31min 31s, total: 1h 8min 47s\n",
      "Wall time: 2h 41min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = Word2Vec(\n",
    "    corpus, # On passe le corpus de ngrams que nous venons de créer\n",
    "    vector_size=32, # Le nombre de dimensions dans lesquelles le contexte des mots devra être réduit, aka. vector_size\n",
    "    window=5, # La taille du \"contexte\", ici 5 mots avant et après le mot observé\n",
    "    min_count=5, # On ignore les mots qui n'apparaissent pas au moins 5 fois dans le corpus\n",
    "    workers=4, # Permet de paralléliser l'entraînement du modèle en 4 threads\n",
    "    epochs=5 # Nombre d'itérations du réseau de neurones sur le jeu de données pour ajuster les paramètres avec la descente de gradient, aka. epochs.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = \"../data/newspapers.model\"\n",
    "model.save(outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remarque\n",
    "\n",
    "Vous voyez ici que l'entrainement du modèle est parallélisé (sur 4 workers).\n",
    "\n",
    "Lors qu'on parallélise l'entrainement du modèle, 4 modèles \"séparés\" sont entrainés sur environ un quart des phrases.\n",
    "\n",
    "Ensuite, les résultats sont agrégés pour ne plus faire qu'un seul modèle.\n",
    "\n",
    "On ne peut prédire quel worker aura quelle phrase, car il y a des aléas lors de la parallélisation (p. ex. un worker qui serait plus lent, etc.).\n",
    "\n",
    "Du coup, les valeurs peuvent varier légèrement d'un entrainement à l'autre.\n",
    "\n",
    "Mais, globalement, les résultats restent cohérents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sauver le modèle dans un fichier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = f\"../data/newspapers.model\"\n",
    "model.save(outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explorer le modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Charger le modèle en mémoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD = /Users/azossieteresafabiola/Tp3/tac/tps/tp3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(\"CWD =\", os.getcwd())  # où tu es exactement\n",
    "for root, dirs, files in os.walk(os.getcwd()):\n",
    "    if \"newspapers.model\" in files:\n",
    "        print(\"FOUND MODEL:\", os.path.join(root, \"newspapers.model\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imprimer le vecteur d'un terme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -1.5731053 ,  -2.4102094 ,   1.1297305 ,   5.180584  ,\n",
       "        -5.0943117 ,  -2.3847933 ,  -7.052009  ,  -4.413669  ,\n",
       "        -4.570704  ,  -2.303178  ,   1.2155125 ,  -2.4840639 ,\n",
       "        -3.191879  ,  -5.892077  ,  -1.0825129 ,   3.6056783 ,\n",
       "        -5.6272354 ,  -3.8113697 ,  -0.17154323,  -8.60715   ,\n",
       "        -5.8219314 ,  -1.3595284 ,   4.434311  ,  -2.591416  ,\n",
       "        -2.3032727 ,  -3.8585687 ,  -2.0520978 ,  -6.152695  ,\n",
       "         4.6079407 , -11.851414  ,  -3.2925901 ,  -1.3984473 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv[\"ministre\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculer la similarité entre deux termes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7236861"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity(\"ministre\", \"roi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chercher les mots les plus proches d'un terme donné"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('premier_ministre', 0.9182556867599487),\n",
       " ('ministre_des_finances', 0.8865057229995728),\n",
       " ('ministere', 0.8847351670265198),\n",
       " ('rapporteur', 0.874431312084198),\n",
       " ('chancelier', 0.8742679953575134),\n",
       " ('president', 0.8686789870262146),\n",
       " ('mi_nistre', 0.8564624190330505),\n",
       " ('ministro', 0.8497983813285828),\n",
       " ('gouverneur_general', 0.8491296768188477),\n",
       " ('ancien_ministre', 0.8409953117370605)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"ministre\", topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faire des recherches complexes à travers l'espace vectoriel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('berlin', 0.936758816242218), ('rome', 0.9005275964736938), ('moscou', 0.8978778719902039), ('geneve', 0.8574203252792358), ('vienne', 0.8552701473236084), ('budapest', 0.8532125949859619), ('alger', 0.8517098426818848), ('prague', 0.8445860743522644), ('washington', 0.8389530777931213), ('stockholm', 0.8375347256660461)]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar(positive=['paris','londres','allemagne'], negative=['belgique'], topn=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('berne', 0.8363231420516968), ('stockholm', 0.8158223032951355), ('berlin', 0.8101850748062134), ('fevrier', 0.7998620271682739), ('decembre', 0.7931808233261108), ('budapest', 0.7913472056388855), ('janvier', 0.7873720526695251), ('octobre', 0.7792397737503052), ('novembre', 0.7785266041755676), ('copenhague', 0.7749161720275879)]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar(positive=['paris', 'londres','allemagne'], negative=['france', 'angleterre'], topn=10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tac_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
